# Predict Poverty Rates & Household Consumption

This project aims to impute household-level daily per capita consumption and estimate population-level poverty rates at 19 different thresholds (from the 5th to the 95th percentile) for new unseen survey datasets.

## User Review Required

> [!IMPORTANT]
> The evaluation metric heavily prioritizes the accuracy of poverty rates (90% weight) over individual household consumption error (10% weight). 
> The proposed approach uses a regression model to predict household consumption, and then calculates the poverty rates deterministically using the survey weights. This is because predicting the continuous consumption variable works best with the given data. 
> Does this approach look sound to you? Alternatively, we could explore a specific loss function that targets the weighted poverty calculation directly, but a standard regression with a robust model like LightGBM/XGBoost is the most solid starting point.

## Proposed Changes

### Machine Learning Pipeline
I will implement the end-to-end data processing and modeling script.

#### [NEW] `model_pipeline.py`
This script will perform the following steps:
1. **Data Loading & Preprocessing:**
   - Merge [train_hh_features.csv](file:///d:/Online%20project/train_hh_features.csv) with [train_hh_gt.csv](file:///d:/Online%20project/train_hh_gt.csv) to map features to the target `cons_ppp17`.
   - Process categorical encodings (one-hot encoding or category types for variables like `dweltyp`, `sector1d`, `region*`).
   - Treat missing values appropriately leveraging tree-based native handling.
2. **Cross-Validation Strategy:**
   - Since the test set consists of entirely new survey contexts, I will use a **Leave-One-Survey-Out** (or GroupKFold by `survey_id`) cross-validation on the training set to properly estimate model generalization.
3. **Model Training:**
   - Train a Gradient Boosting Regressor (e.g., LightGBM or XGBoost) on the household features to predict `cons_ppp17`.
4. **Poverty Rate Calculation:**
   - For a given set of predictions and given threshold `T` (e.g. `_pline50`), compute the population poverty rate as: [( (predictions < T) * weights ).sum() / weights.sum()](file:///d:/Online%20project/app.py#17-28).
5. **Prediction & Submission Generation:**
   - Apply the pipeline to [test_hh_features.csv](file:///d:/Online%20project/test_hh_features.csv).
   - Output `predicted_household_consumption.csv` (with `survey_id`, `hhid`, and predicted consumption).
   - Output `predicted_poverty_distribution.csv` (with `survey_id` and the 19 threshold rates).

## Verification Plan

### Automated Tests
- I will run the cross-validation script locally to evaluate performance against the 3 training surveys. The script will output the average estimation error on both household consumption and poverty rates.
- I will run the inference script to ensure `predicted_household_consumption.csv` and `predicted_poverty_distribution.csv` are successfully generated without errors.

### Manual Verification
- You can manually inspect the out-of-fold cross-validation metrics and check if the distribution of our predicted poverty rates matches the [train_rates_gt.csv](file:///d:/Online%20project/train_rates_gt.csv) closely before relying on the test submissions.
